{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a690de4-5cdd-4098-bb59-2ebbfa753148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\ProgramData\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\win\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import (\n",
    "\tOneHotEncoder,\n",
    "\tOrdinalEncoder,\n",
    "\tStandardScaler,\n",
    "\tMinMaxScaler,\n",
    "\tPowerTransformer,\n",
    "\tFunctionTransformer\n",
    ")\n",
    "\n",
    "from feature_engine.outliers import Winsorizer\n",
    "from feature_engine.datetime import DatetimeFeatures\n",
    "from feature_engine.selection import SelectBySingleFeaturePerformance\n",
    "from feature_engine.encoding import (\n",
    "\tRareLabelEncoder,\n",
    "\tMeanEncoder,\n",
    "\tCountFrequencyEncoder\n",
    ")\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a956017-24dd-400e-a81d-5c129d04a521",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f87edb43-5185-435c-a5f1-a295d34aff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.set_config(transform_output=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed5c661d-3220-4e6c-8d54-4f3891aac90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b66a057-7a08-48fe-a744-c64c3b54c4fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m train\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b7e3fe1-074a-4ddc-8573-d9b2694cd0c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m val \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m val\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "val = pd.read_csv(\"val.csv\")\n",
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7aa7753-baa0-4e65-935d-42fc6a596580",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m test\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(\"test.csv\")\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ee3ce80-d48e-4bde-9c9e-c4251a618f1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# airline\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m air_transformer \u001b[38;5;241m=\u001b[39m Pipeline(steps\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      3\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimputer\u001b[39m\u001b[38;5;124m\"\u001b[39m, SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmost_frequent\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[0;32m      4\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrouper\u001b[39m\u001b[38;5;124m\"\u001b[39m, RareLabelEncoder(tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, replace_with\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOther\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_categories\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)),\n\u001b[0;32m      5\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m\"\u001b[39m, OneHotEncoder(sparse_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, handle_unknown\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      6\u001b[0m ])\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#doj\u001b[39;00m\n\u001b[0;32m      9\u001b[0m feature_to_extract \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweek\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mday_of_week\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mday_of_year\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# airline\n",
    "air_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"grouper\", RareLabelEncoder(tol=0.1, replace_with=\"Other\", n_categories=2)),\n",
    "    (\"encoder\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "#doj\n",
    "feature_to_extract = [\"month\", \"week\", \"day_of_week\", \"day_of_year\"]\n",
    "\n",
    "doj_transformer = Pipeline(steps=[\n",
    "    (\"dt\", DatetimeFeatures(features_to_extract=feature_to_extract, yearfirst=True, format=\"mixed\")),\n",
    "    (\"scaler\", MinMaxScaler())\n",
    "])\n",
    "\n",
    "# source & destination\n",
    "location_pipe1 = Pipeline(steps=[\n",
    "    (\"grouper\", RareLabelEncoder(tol=0.1, replace_with=\"Other\", n_categories=2)),\n",
    "    (\"encoder\", MeanEncoder()),\n",
    "    (\"scaler\", PowerTransformer())\n",
    "])\n",
    "\n",
    "def is_north(X):\n",
    "    columns = X.columns.to_list()\n",
    "    north_cities = [\"Delhi\", \"Kolkata\", \"Mumbai\", \"New Delhi\"]\n",
    "    return (\n",
    "        X\n",
    "        .assign(**{\n",
    "            f\"{col}_is_north\": X.loc[:, col].isin(north_cities).astype(int)\n",
    "            for col in columns\n",
    "        })\n",
    "    .drop(columns=columns)\n",
    "    )\n",
    "\n",
    "location_transformer = FeatureUnion(transformer_list=[\n",
    "    (\"part1\", location_pipe1),\n",
    "    (\"part2\", FunctionTransformer(func=is_north))\n",
    "])\n",
    "\n",
    "# dep_time & arrival_time\n",
    "time_pipe1 = Pipeline(steps=[\n",
    "    (\"dt\", DatetimeFeatures(features_to_extract=[\"hour\", \"minute\"])),\n",
    "    (\"scaler\", MinMaxScaler())\n",
    "])\n",
    "\n",
    "def part_of_day(X, morning=4, noon=12, eve=16, night=20):\n",
    "    columns = X.columns.to_list()\n",
    "    X_temp = X.assign(**{\n",
    "        col: pd.to_datetime(X.loc[:, col]).dt.hour\n",
    "        for col in columns\n",
    "    })\n",
    "\n",
    "    return (\n",
    "        X_temp\n",
    "        .assign(**{\n",
    "            f\"{col}_part_of_day\": np.select(\n",
    "                [X_temp.loc[:, col].between(morning, noon, inclusive=\"left\"),\n",
    "                 X_temp.loc[:, col].between(noon, eve, inclusive=\"left\"),\n",
    "                 X_temp.loc[:, col].between(eve, night, inclusive=\"left\")],\n",
    "                [\"morning\", \"afternoon\", \"evening\"],\n",
    "                default=\"night\"\n",
    "     )\n",
    "            for col in columns\n",
    "        })\n",
    "        .drop(columns=columns)\n",
    "    )\n",
    "\n",
    "time_pipe2 = Pipeline(steps=[\n",
    "    (\"part\", FunctionTransformer(func=part_of_day)),\n",
    "    (\"encoder\", CountFrequencyEncoder()),\n",
    "    (\"scaler\", MinMaxScaler())\n",
    "])\n",
    "\n",
    "time_transformer = FeatureUnion(transformer_list=[\n",
    "    (\"part1\", time_pipe1),\n",
    "    (\"part2\", time_pipe2)\n",
    "])\n",
    "\n",
    "# duration\n",
    "class RBFPercentileSimilarity(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, variables=None, percentiles=[0.25, 0.5, 0.75], gamma=0.1):\n",
    "        self.variables = variables\n",
    "        self.percentiles = percentiles\n",
    "        self.gamma = gamma\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if not self.variables:\n",
    "            self.variables = X.select_dtypes(include=\"number\").columns.to_list()\n",
    "\n",
    "        self.reference_values_ = {\n",
    "            col: (\n",
    "                X\n",
    "        .loc[:, col]\n",
    "                .quantile(self.percentiles)\n",
    "                .values\n",
    "                .reshape(-1, 1)\n",
    "            )\n",
    "            for col in self.variables\n",
    "        }\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, X):\n",
    "        objects = []\n",
    "        for col in self.variables:\n",
    "            columns = [f\"{col}_rbf_{int(percentile * 100)}\" for percentile in self.percentiles]\n",
    "            obj = pd.DataFrame(\n",
    "                data=rbf_kernel(X.loc[:, [col]], Y=self.reference_values_[col], gamma=self.gamma),\n",
    "                columns=columns\n",
    "            )\n",
    "            objects.append(obj)\n",
    "        return pd.concat(objects, axis=1)\n",
    "    \n",
    "\n",
    "def duration_category(X, short=180, med=400):\n",
    "    return (\n",
    "        X\n",
    "        .assign(duration_cat=np.select([X.duration.lt(short),\n",
    "                                    X.duration.between(short, med, inclusive=\"left\")],\n",
    "                                    [\"short\", \"medium\"],\n",
    "                                    default=\"long\"))\n",
    "        .drop(columns=\"duration\")\n",
    "         )\n",
    "\n",
    "def is_over(X, value=1000):\n",
    "    return (\n",
    "        X\n",
    "        .assign(**{\n",
    "            f\"duration_over_{value}\": X.duration.ge(value).astype(int)\n",
    "        })\n",
    "        .drop(columns=\"duration\")\n",
    "    )\n",
    "\n",
    "duration_pipe1 = Pipeline(steps=[\n",
    "    (\"rbf\", RBFPercentileSimilarity()),\n",
    "    (\"scaler\", PowerTransformer())\n",
    "])\n",
    "\n",
    "duration_pipe2 = Pipeline(steps=[\n",
    "    (\"cat\", FunctionTransformer(func=duration_category)),\n",
    "    (\"encoder\", OrdinalEncoder(categories=[[\"short\", \"medium\", \"long\"]]))\n",
    "])\n",
    "\n",
    "duration_union = FeatureUnion(transformer_list=[\n",
    "    (\"part1\", duration_pipe1),\n",
    "    (\"part2\", duration_pipe2),\n",
    "    (\"part3\", FunctionTransformer(func=is_over)),\n",
    "    (\"part4\", StandardScaler())\n",
    "])\n",
    "\n",
    "duration_transformer = Pipeline(steps=[\n",
    "    (\"outliers\", Winsorizer(capping_method=\"iqr\", fold=1.5)),\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"union\", duration_union)\n",
    "])\n",
    "\n",
    "# total_stops\n",
    "def is_direct(X):\n",
    "    return X.assign(is_direct_flight=X.total_stops.eq(0).astype(int))\n",
    "\n",
    "\n",
    "total_stops_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"\", FunctionTransformer(func=is_direct))\n",
    "])\n",
    "\n",
    "# additional_info\n",
    "info_pipe1 = Pipeline(steps=[\n",
    "    (\"group\", RareLabelEncoder(tol=0.1, n_categories=2, replace_with=\"Other\")),\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "def have_info(X):\n",
    "    return X.assign(additional_info=X.additional_info.ne(\"No Info\").astype(int))\n",
    "\n",
    "info_union = FeatureUnion(transformer_list=[\n",
    "(\"part1\", info_pipe1),\n",
    "(\"part2\", FunctionTransformer(func=have_info))\n",
    "])\n",
    "\n",
    "info_transformer = Pipeline(steps=[\n",
    "(\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"unknown\")),\n",
    "(\"union\", info_union)\n",
    "])\n",
    "# column transformer\n",
    "column_transformer = ColumnTransformer(transformers=[\n",
    "(\"air\", air_transformer, [\"airline\"]),\n",
    "(\"doj\", doj_transformer, [\"date_of_journey\"]),\n",
    "(\"location\", location_transformer, [\"source\", 'destination']),\n",
    "(\"time\", time_transformer, [\"dep_time\", \"arrival_time\"]),\n",
    "(\"dur\", duration_transformer, [\"duration\"]),\n",
    "(\"stops\", total_stops_transformer, [\"total_stops\"]),\n",
    "(\"info\", info_transformer, [\"additional_info\"])\n",
    "], remainder=\"passthrough\")\n",
    "\n",
    "# feature selector\n",
    "estimator = RandomForestRegressor(n_estimators=10, max_depth=3, random_state=42)\n",
    "\n",
    "selector = SelectBySingleFeaturePerformance(\n",
    "estimator=estimator,\n",
    "scoring=\"r2\",\n",
    "threshold=0.1\n",
    ") \n",
    "\n",
    "# preprocessor\n",
    "preprocessor = Pipeline(steps=[\n",
    "(\"ct\", column_transformer),\n",
    "(\"selector\", selector)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fab2362-050a-4fa9-bcaf-f0a48419d4f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m preprocessor\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m      2\u001b[0m     train\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      3\u001b[0m     train\u001b[38;5;241m.\u001b[39mprice\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m      4\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocessor' is not defined"
     ]
    }
   ],
   "source": [
    "preprocessor.fit(\n",
    "    train.drop(columns=\"price\"),\n",
    "    train.price.copy()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07f56535-bf17-4304-a23c-c0d3534f30f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m preprocessor\u001b[38;5;241m.\u001b[39mtransform(train\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocessor' is not defined"
     ]
    }
   ],
   "source": [
    "preprocessor.transform(train.drop(columns=\"price\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b94ba3-6290-4924-99ee-c94b8eec262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"sagemaker-flights-bucket\"\n",
    "\n",
    "DATA_PREFIX = \"data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11de07f0-6024-403c-b0db-b0c0e60edffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_name(name):\n",
    "    return f\"{name}-pre.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09652d8-1ce1-4be4-8b4f-465666527709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_data(data, name, pre):\n",
    "    # split data into X and y subsets\n",
    "    X = data.drop(columns=\"price\")\n",
    "    y = data.price.copy()\n",
    "    \n",
    "    # transformation\n",
    "    X_pre = pre.transform(X)\n",
    "    \n",
    "    # exporting\n",
    "    file_name = get_file_name(name)\n",
    "    (\n",
    "        y\n",
    "        .to_frame()\n",
    "        .join(X_pre)\n",
    "        .to_csv(file_name, index=False, header=False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d5ac5b-350e-47a0-813b-eed139b1dac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_bucket(name):\n",
    "    file_name = get_file_name(name)\n",
    "    \n",
    "    (\n",
    "        boto3\n",
    "        .Session()\n",
    "        .resource(\"s3\")\n",
    "        .Bucket(BUCKET_NAME)\n",
    "        .Object(os.path.join(DATA_PREFIX, f\"{name}/{name}.csv\"))\n",
    "        .upload_file(file_name)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5d83b6-990f-4573-9272-cadd7328a963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_and_upload_bucket(data, name, pre):\n",
    "    export_data(data, name, pre)\n",
    "    upload_to_bucket(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16791dd-48b7-4dd6-93e4-d85f10009abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_and_upload_bucket(train, \"train\", preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5ddee7-2d21-420d-abdf-2429594d3b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_and_upload_bucket(val, \"val\", preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1a4133-9f51-4205-b5f6-33b3d948e741",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_and_upload_bucket(test, \"test\", preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bfadfd-5547-45b7-91bf-f8e82323be53",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sagemaker.Session()\n",
    "region_name = session.boto_region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccbe529-9749-4840-88d3-f2a0601b8587",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Estimator(\n",
    "    image_uri=sagemaker.image_uris.retrieve(\"xgboost\", region_name, \"1.2-1\"),\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    volume_size=5,\n",
    "    output_path=output_path,\n",
    "    use_spot_instances=True,\n",
    "    max_run=300,\n",
    "    max_wait=600,\n",
    "    sagemaker_session=session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0afe3ea-02a0-4a62-8916-ffe439833e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_hyperparameters(\n",
    "    objective=\"reg:linear\",\n",
    "    num_round=10,\n",
    "    eta=0.1,\n",
    "    max_depth=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    alpha=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a70624-6720-40c3-9ed3-a075c200bd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {\n",
    "    \"eta\": ContinuousParameter(0.05, 0.2),\n",
    "    \"alpha\": ContinuousParameter(0, 1),\n",
    "    \"max_depth\": IntegerParameter(3, 5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01c411e-4b18-469a-8878-fc6df6524921",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(\n",
    "    estimator=model,\n",
    "    objective_metric_name=\"validation:rmse\",\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    strategy=\"Bayesian\",\n",
    "    objective_type=\"Minimize\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b5b7be-8a7a-4a07-a1df-e3510ae2eda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_channel(name):\n",
    "    bucket_path = f\"s3://{BUCKET_NAME}/{DATA_PREFIX}/{name}\"\n",
    "    return TrainingInput(bucket_path, content_type=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d171ac61-9d4b-4e1f-87da-660c788a6cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_channel = get_data_channel(\"train\")\n",
    "train_data_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e869e1c8-5c3a-4a66-aa24-0434af56d24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_channel = get_data_channel(\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a041eb4f-820d-409c-87e9-28dff794a6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_channels = {\n",
    "    \"train\": train_data_channel,\n",
    "    \"validation\": val_data_channel\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47a10fb-6966-4c57-9bdb-00346575c9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.fit(data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5aeda1-134d-47a5-be86-4748758ef383",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"xgboost-model\", \"rb\") as f:\n",
    "    best_model = pickle.load(f)\n",
    "    \n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cddcbb-f51d-4294-ae9b-08f8dc5f7f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(name):\n",
    "    file_name = get_file_name(name)\n",
    "    data = pd.read_csv(file_name)\n",
    "    \n",
    "    X = xgb.DMatrix(data.iloc[:, 1:])\n",
    "    y = data.iloc[:, 0].copy()\n",
    "    \n",
    "    pred = best_model.predict(X)\n",
    "    \n",
    "    return r2_score(y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19295b19-8aed-44df-ad8c-584361f89222",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992d55a5-7cb0-4342-9a9f-0e0b3cbf6cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc4d163-c47e-470c-b565-b37313f431c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(\"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
